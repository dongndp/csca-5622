{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning - House Price Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Vietnam is a developing country in Southeast Asia, with a population of about 100 million people and is one of the fastest growing economies in recent years. Along with rapid economic development and urbanization, housing price is also increased repidly and become too expensive for most people. Housing price is always one of the issues of great concern to many families.\n",
    "\n",
    "In this assignment, I'm going to build a simple supervised learning model to predict housing prices in Ho Chi Minh city, the economic and largest city in Vietnam. For the purpose of price prediction, I'm going to use sklearn regression methods. Various analytical techniques are also used to explore and clean data.\n",
    "\n",
    "### About the dataset:\n",
    "\n",
    "I will be using the  **'House Price Prediction Dataset Vietnam - 2024'** dataset which was downloaded from Kaggle (https://www.kaggle.com/datasets/nguyentiennhan/vietnam-housing-dataset-2024/data) for the purpose of this study.\n",
    "\n",
    "According to the dataset description from Kaggle's website: This dataset contains information about various housing properties in Vietnam. It includes detailed attributes of each property, such as its location, physical characteristics, and legal and furnishing status, along with the price. \n",
    "\n",
    "The data was crawled from batdongsan.vn, which is one of the largest real estate listing websites in Vietnam.\n",
    "\n",
    "Features:\n",
    "* Address: The complete address of the property, including details such as the project name, street, ward, district, and city.\n",
    "* Area: The total area of the property, measured in square meters.\n",
    "* Frontage: The width of the front side of the property, measured in meters.\n",
    "* Access Road: The width of the road providing access to the property, measured in meters.\n",
    "* House Direction: The cardinal direction the front of the house is facing (e.g., East, West, North, South).\n",
    "* Balcony Direction: The cardinal direction the balcony is facing.\n",
    "* Floors: The total number of floors in the property.\n",
    "* Bedrooms: The number of bedrooms in the property.\n",
    "* Bathrooms: The number of bathrooms in the property.\n",
    "* Legal Status: Indicates the legal status of the property, such as whether it has a certificate of ownership or is under a sale contract.\n",
    "* Furniture State: Indicates the state of furnishing in the property, such as fully furnished, partially furnished, or unfurnished.\n",
    "\n",
    "Target: \n",
    "* Price: The price of the property, represented in billions of Vietnamese Dong (VND).\n",
    "\n",
    "### GitHub repository\n",
    "\n",
    "[https://github.com/dongndp/csca-5622](https://github.com/dongndp/csca-5622)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Exploration & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "df = pd.read_csv('data/vietnam_housing_dataset.csv')\n",
    "# Replace all spaces in colum names with hyphens\n",
    "df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "# Display information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First observations: The dataset contains 30229 samples, 11 features and 1 target variable. Among the 11 features, there are 6 numeric and 5 categorical features. \n",
    "There are many features with missing values. Let's look at a few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see more detail about address\n",
    "for i in range(5):\n",
    "    print(df['Address'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, the Address field is a combination of the construction project name, buldings, street, town/ward, city/district and city/province separated by commas (,). I'm going to remove all house entries that are not located in Ho Chi Minh city (since I just want to predict house prices in Ho Chi Minh city only) and create new column District and then drop the original Address column as I don't need it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Address'].str.split(\", \").str[-1].str.replace(\".\", \"\") == \"Hồ Chí Minh\"]\n",
    "df['District'] = df['Address'].str.split(\", \").str[-2].str.replace(\".\", \"\")\n",
    "df.drop('Address', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to look at 'District'\n",
    "df['District'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to drop all districts which have less than 10 entries. I believe the district name of these entries are not input correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_district_samples = 10\n",
    "\n",
    "unique_districts = df['District'].value_counts()\n",
    "unique_districts = unique_districts[unique_districts >= min_district_samples].index\n",
    "df = df[df['District'].isin(unique_districts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null/missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect house direction feature\n",
    "df['House_direction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check balcony direction\n",
    "df['Balcony_direction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The House and Balcony direction are categorical features. They indicate the cardinal direction of the house such as East (Đông), West (Tây), South (Nam), North (Bắc), South - East (Đông Nam), North - West (Tây Bắc), etc. While house orientation is one of the factors that influence buyers' decisions, it has little or no impact on home prices. And since there are so many missing values, I decide to drop these two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop House/Balcony direction features\n",
    "df.drop('House_direction', inplace = True, axis = 1)\n",
    "df.drop('Balcony_direction', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems not make sense for a house or apartment with zero floor, bedroom, bathroom or no access road... So for any numeric feature, I'm going to fill the missing value with the median. Likewise for any categorical feature, I'm going to fill the missing value with the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "for col in ['Floors', 'Frontage', 'Access_Road', 'Bedrooms', 'Bathrooms']: \n",
    "    fill_value = df[col].median()\n",
    "    df.fillna({col: fill_value}, inplace=True)\n",
    "\n",
    "for col in ['Legal_status', 'Furniture_state']:\n",
    "    fill_value = df[col].mode()\n",
    "    df.fillna({col: fill_value[0]}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Initial Exploration and Data Cleaning\n",
    "\n",
    "The original dataset contains 30.227 samples, 11 features. There are many entries with null values.\n",
    "\n",
    "What I have done so far:\n",
    "* Remove all house entries which are not located in Ho Chi Minh city since I want to predict the housing prices in Ho Chi Minh city only.\n",
    "* Remove all features that contain too many null values and does not make sense to keep them, including the House and Balcony orientations.\n",
    "* Fill all missing values with the median (for numerical features) and the mode (for categorical features)\n",
    "\n",
    "After all of above cleaning tasks, now I have a clean dataset which contains 11.751 samples of listing houses in Ho Chi Minh city, each has 9 features: Area, Frontage, Access Road, Floors, Number of Bedrooms, Number of Bathrooms, Legal status, Furniture state, and District where it's located. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Above, I assumed that house prices vary by province and district. I'm going to check if the data supports my hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a box plot to see if house prices vary by district\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "fig.suptitle('Price vs District')\n",
    "sns.boxplot(x='District', y='Price', data=df, ax=ax)\n",
    "ax.set(ylabel='Price (Billion VND)', xlabel='District')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the box plot results above, I can conclude that the data supports my assumption. A house located in downtown (Quận 1) or new areas such as Quận 2 tend to be higher than on located in surbubs such as Củ Chi or Bình Chánh.\n",
    "\n",
    "From the plot, there are some outliers. I assume that these are true ouliers and I'm not going to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Distribution\n",
    "\n",
    "Now I want to see how the distribution of prices looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the distribution of prices\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "fig.suptitle('House price distribution')\n",
    "sns.histplot(df['Price'], kde=True, ax=ax)\n",
    "ax.set(xlabel='Price (Billion VND)', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of prices looks a little bit skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap\n",
    "\n",
    " Let's view the correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns and display the correlation heatmap\n",
    "df1 = df.select_dtypes(include=[np.number])\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(df1.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap shows that Bathrooms, Bedrooms, Floors and Areas are correlated with the target variable. It also shows that Bathrooms and Bedrooms are strongly correlated (0.80), Bathrooms and Floors (0.61), Bedrooms and Floors (0.54) are also correlated with each others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of EDA\n",
    "\n",
    "What I've found so far:\n",
    "\n",
    "* The house prices vary by areas, houses located in the downtown or new areas are higher than in the surbubs\n",
    "* The price distribution of house prices is a bit skewed\n",
    "* Bathrooms, Bedrooms Floors, and Area are correlated with the price, but not strongly, while Fontage isn't.\n",
    "* Bathrooms and Bedrooms are strongly correlated, Bathrooms and Floors, Bedrooms and Floors are also correlated with each others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical variables to numerical variables and prepare train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Converts categorical variables into dummy/indicator variables\n",
    "df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "\n",
    "# split into train and test dataset\n",
    "X = df.drop(columns=['Price'])\n",
    "y = df['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "lr_model = LinearRegression().fit(X_train, y_train)\n",
    "lr_score = lr_model.score(X_test, y_test)\n",
    "lr_y_pred = lr_model.predict(X_test)\n",
    "lr_rmse = root_mean_squared_error(y_test, lr_y_pred)\n",
    "lr_residuals = y_test - lr_y_pred\n",
    "print(f\"Score: {lr_score:.2f}, RMSE: {lr_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Regressor\n",
    "\n",
    "The linear regression model gave the score of 0.52 and RMSE=1.44. In the following I'm going to use RandomForest Regressor to see if it can provide better score. I also try to use GridSearchCV to test the hyper-parameters and see which is the best choice of hyper-parameters that gives the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = {'max_depth': [1, 5, 10, 15, 20, 25, 30]}\n",
    "base_estimator = RandomForestRegressor(max_features='sqrt')\n",
    "clf = GridSearchCV(base_estimator, param_grid, cv=5).fit(X_train, y_train)\n",
    "rf_model = clf.best_estimator_\n",
    "rf_params = clf.best_params_\n",
    "rf_score = rf_model.score(X_test, y_test)\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "rf_residuals = y_test - rf_y_pred\n",
    "rf_rmse = root_mean_squared_error(y_test, rf_y_pred)\n",
    "print(f\"Best params: {rf_params}, Best score: {rf_score:.2f}, Best RMSE: {rf_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result shown, the RandomForest Regressor model gave a score of 0.64 and RMSE=1.25 which is much better than previous Linear Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(10, 4))\n",
    "fig.suptitle('Histogram of Residuals')\n",
    "sns.histplot(lr_residuals, kde=True, ax=ax1)\n",
    "ax1.set(xlabel=f'Residual (LinearRegresstion), RMSE={lr_rmse:.2f}', ylabel='Frequency')\n",
    "sns.histplot(rf_residuals, kde=True, ax=ax2)\n",
    "ax2.set(xlabel=f'Residual (RandomForestRegressor), RMSE={rf_rmse:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted vs Actual values plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(10, 4))\n",
    "fig.suptitle('Predicted vs Actual Values')\n",
    "ax1.scatter(y_test, lr_y_pred, alpha=0.5)\n",
    "ax1.plot([1, 10], [1, 10], 'r--')\n",
    "ax1.set(xlabel='Actual Value (LinearRegresstion)', ylabel='Predicted Value (Billion VND)')\n",
    "ax2.scatter(y_test, rf_y_pred, alpha=0.5)\n",
    "ax2.plot([1, 10], [1, 10], 'r--')\n",
    "ax2.set(xlabel='Actual Value (RandomForestRegressor)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, I explored the Vietnam housing price dataset, picked up houses in Ho Chi Minh city, performed data cleaning tasks such as removing features that have too many missing values or fill missing values with median/mode value. \n",
    "\n",
    "During EDA, I looked at the data distribution, I assumed the outliers are true outliers and decided to not removing them. I plotted the correlation heatmap and analyzed the correlation between features and target variable. \n",
    "\n",
    "I prepared the data before test my models by converting all categorical features to numerical and then splitted the data into train and test datasets. \n",
    "\n",
    "Finally, I tested with 2 sklearn's regression models: LinearRegresstion and RandomForestRegressor. For the later model, I also tried different values of hyper-parameters using GridSearchCV() to find the model that gives the highest score. As the result shown, RandomForestRegressor gives the score of 0.64 which is much better than LinearRegression model. But I believe that the model's can be further improved by considering to remmove outliers, tunning model's hyper-parameters or exploring more models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
